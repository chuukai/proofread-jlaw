{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "publicbunruiwindow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1CKPrfhi4EQbwBUu_a5DZxmX2aX-zV8A5",
      "authorship_tag": "ABX9TyNamsRLcxWOJ488ZSk90HSw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "t49bdYB6rO7J"
      },
      "source": [
        "# 形態素分析ライブラリーMeCab と 辞書(mecab-ipadic-NEologd)のインストール \n",
        "!apt-get -q -y install sudo file mecab libmecab-dev mecab-ipadic-utf8 git curl python-mecab > /dev/null\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null \n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n > /dev/null 2>&1\n",
        "!pip install mecab-python3 > /dev/null\n",
        " \n",
        "# シンボリックリンクによるエラー回避\n",
        "!ln -s /etc/mecabrc /usr/local/etc/mecabrc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqov-foor6Su"
      },
      "source": [
        "import os\n",
        "import MeCab\n",
        "import re\n",
        "import subprocess\n",
        "import sys\n",
        "path = \"-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd -b 819200\"\n",
        "m = MeCab.Tagger(path)\n",
        "noun_list = ''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "for bunrui in range(1,51):\n",
        "  lssentence = \" \".join(['ls','/content/drive/My\\ Drive/Colab\\ Notebooks/houreibunrui20210510/%02d/*.nml' % bunrui])\n",
        "  output = subprocess.getoutput(lssentence)\n",
        "  tolists = output.split('\\n')\n",
        "  for tolist in tolists:\n",
        "    print(tolist)\n",
        "    text_data = open(tolist, \"rb\").read()\n",
        "    decoded_text = text_data.decode('utf-8')\n",
        "    text = re.sub('\\n\\n', '\\n', decoded_text)\n",
        "    with open(tolist + '.wwakati', 'a') as f:\n",
        "      parses = m.parse(text)\n",
        "      if parses is None:\n",
        "        tolistformecab = re.sub(\" \", \"\\ \", tolist)\n",
        "        mecabsentence = \" \".join(['cat', tolistformecab, '|', 'mecab', path])\n",
        "        parses = subprocess.getoutput(mecabsentence)\n",
        "      t_parses = parses.split(os.linesep)\n",
        "      for t_parse in t_parses:\n",
        "        t_lists = t_parse.split('\\t')\n",
        "        term = t_lists[0] + ' '\n",
        "        if t_lists[0] == 'EOS' or t_lists[0] == '' or t_lists[0] == 'と' or t_lists[0] == 'の':\n",
        "          term = ''\n",
        "          continue\n",
        "        for hinsi in ['記号', '助詞,連体化', '助詞,副詞化', '助詞,係助詞', '助詞,接続助詞', '助詞,並立助詞', '助詞,副助詞／並立助詞／終助詞', '助詞,格助詞,引用', '助詞,格助詞,一般,*,*,*,へ,ヘ,エ', '助詞,格助詞,一般,*,*,*,に,ニ,ニ', '助詞,格助詞,一般,*,*,*,を,ヲ,ヲ', '助詞,格助詞,一般,*,*,*,で,デ,デ', '助詞,格助詞,一般,*,*,*,が,ガ,ガ', '助詞,格助詞,一般,*,*,*,から,カラ,カラ', '助詞,副助詞,*,*,*,*,まで,マデ,マデ', '連体詞', '接頭詞,名詞接続', '特殊', '形容詞,自立,*,*,形容詞・アウオ段,基本形,ない,ナイ,ナイ', '接頭詞,数接続', '名詞,数', '名詞,接尾', '名詞,非自立,副詞可能', '名詞,非自立,助動詞語幹', '名詞,接尾,助数詞', '名詞,代名詞', '助動詞', '名詞,非自立,一般', '動詞,非自立', '動詞,自立,*,*,サ変・スル', '動詞,接尾', '動詞,自立,*,*,一段,基本形,できる,デキル,デキル', '動詞,自立,*,*,五段・ラ行,基本形,係る,カカル,カカル', '助詞,格助詞,連語,*,*,*,に']:\n",
        "          if t_lists[1].find(hinsi) > -1:\n",
        "            term = ''\n",
        "        for stopword in ['ヶ所', 'ヵ所', 'カ所', '箇所', 'ヶ月', 'ヵ月', 'カ月', '箇月', '十', '百', '千', '万', '億', '兆', 'もの', '明治', '大正', '昭和', '平成', '令和']:\n",
        "          if t_lists[0].find(stopword) > -1:\n",
        "            term = ''\n",
        "        noun_list = noun_list + term\n",
        "      f.write(noun_list)\n",
        "      noun_list = ''\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08SZau1eP40V"
      },
      "source": [
        "!find /content/drive/My\\ Drive/Colab\\ Notebooks/houreibunrui20210510/ -name \"*.wwakati\" -type f -printf \"%p,%s\\n\" > /content/drive/My\\ Drive/Colab\\ Notebooks/houreibunrui20210510/bunruiallw.txt\n",
        "# 法令ファイルには既施行と未施行が重複している場合があるため、ファイル名の前半部分を使って重複を取り除き、bunruiwall-sort-uniq2.csvを作成する。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQDJrSaNr689"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import re\n",
        "import subprocess\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "f=open('/content/drive/My Drive/Colab Notebooks/houreibunrui20210510/bunruiwall-sort-uniq2.csv')\n",
        "lines=f.readlines()\n",
        "f.close()\n",
        "bfs = []\n",
        "sums= []\n",
        "nums = []\n",
        "mods = []\n",
        "bunrui = 0\n",
        "bunruii = 1\n",
        "#以下の1690000は、2160000ではメモリ不足になる場合に使用している。通常は2160000でよい。\n",
        "for line in lines:\n",
        "  bunrui = int(line[0:2])\n",
        "  fn = line[3:64]\n",
        "  size = int(line[65:].replace(\"\\n\", \"\"))\n",
        "  bfs.append((bunrui, fn, size))\n",
        "sums = [0, 0]\n",
        "nums = [0]\n",
        "mods = [0]\n",
        "bunruilens = [0]\n",
        "bunruilen = 0\n",
        "for bfsi in bfs:\n",
        "  if bfsi[0]  == bunruii:\n",
        "    sums[bunruii] = sums[bunruii] + bfsi[2]\n",
        "    bunruilen += 1\n",
        "  elif sums[bunruii]//1690000  == 0:\n",
        "    nums.append(1)\n",
        "    mods.append(0)\n",
        "    sums.append(bfsi[2])\n",
        "    bunruilens.append(bunruilen)\n",
        "    print(bunruii, 1, 0, bunruilen)\n",
        "    bunruilen = 1\n",
        "    bunruii = bfsi[0]\n",
        "  else:\n",
        "    nums.append(sums[bunruii]//1690000 + 1)\n",
        "    mods.append(((sums[bunruii] % -1690000)//(nums[bunruii] - 1)) * -1)\n",
        "    sums.append(bfsi[2])\n",
        "    bunruilens.append(bunruilen)\n",
        "    print(bunruii, sums[bunruii], sums[bunruii]//1690000 + 1, ((sums[bunruii] % -1690000)//(nums[bunruii] - 1)) * -1, bunruilen)\n",
        "    bunruilen = 1\n",
        "    bunruii = bfsi[0]\n",
        "if sums[bunruii]//1690000  == 0:\n",
        "  nums.append(1)\n",
        "  mods.append(0)\n",
        "  bunruilens.append(bunruilen)\n",
        "  print(bunruii, 1, 0)\n",
        "else:\n",
        "  nums.append(sums[bunruii]//1690000 + 1)\n",
        "  mods.append(((sums[bunruii] % -1690000)//(nums[bunruii] - 1)) * -1)\n",
        "  bunruilens.append(bunruilen)\n",
        "  print(bunruii, sums[bunruii]//1690000 + 1, ((sums[bunruii] % -1690000)//(nums[bunruii] - 1)) * -1, bunruilen)\n",
        "print(len(sums), len(nums), len(mods), len(bunruilens))\n",
        "print(bunruilens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmMwIngvnVeR"
      },
      "source": [
        "wakati = ['']\n",
        "wakatilist = []\n",
        "wakatilistbw = []\n",
        "numw = 0\n",
        "sumfw = 0\n",
        "sumbw = 0\n",
        "t = 0\n",
        "outlist = [11]\n",
        "def writeallwakati(bunruii, numw, wakatilist):\n",
        "  fn = '/content/drive/My Drive/Colab Notebooks/houreibunrui20210510/01-50wakatiall/%02d-%02dw.wwakatiall' % (bunruii, numw)\n",
        "  with open(fn, 'a') as fa:\n",
        "    for tolist in wakatilist:\n",
        "      if not tolist == '':\n",
        "        text_data = open('/content/drive/My Drive/Colab Notebooks/houreibunrui20210510/%02d/' % bunruii + tolist[0], \"rb\").read()\n",
        "        text = text_data.decode('utf-8')\n",
        "      fa.write(text)\n",
        " \n",
        "for bunruii in range(1, 51):\n",
        "  start = sum(bunruilens[0:bunruii])\n",
        "  end = sum(bunruilens[0:bunruii + 1]) - 1\n",
        "  print(bunruii, start, end)\n",
        "  for t1, bfsi in enumerate(bfs[start:end + 1]):\n",
        "    sumfw = sumfw + int(bfsi[2])\n",
        "    if sumfw > 1590000:\n",
        "      print('reversed', bunruii, sumfw)\n",
        "      wakati.append((bunruii, numw, wakatilist))\n",
        "      writeallwakati(bunruii, numw, wakatilist)\n",
        "      wakatilist = []\n",
        "      wakatilistbw = []\n",
        "      numw += 1\n",
        "      sumfw = 0\n",
        "      for t2, bfsbw in enumerate(reversed(bfs[start:start + t1])):\n",
        "        sumbw = sumbw + int(bfsbw[2])\n",
        "        print(sumbw, int(mods[bunruii])/(int(nums[bunruii]) - 1))\n",
        "        if sumbw > int(mods[bunruii])/(int(nums[bunruii]) - 1):\n",
        "          sumfw = sumbw - int(bfsbw[2]) + int(bfsi[2]) \n",
        "          print('sumfw is %d' % sumfw)\n",
        "          wakatilist = wakatilistbw[:]\n",
        "          wakatilistbw = []\n",
        "          sumbw = 0\n",
        "          break\n",
        "        wakatilistbw.append((bfsbw[1],bfsbw[2]))\n",
        "        print(t2, sumbw)\n",
        "      else:\n",
        "        sumfw = sumbw + int(bfsi[2]) \n",
        "        print('sumbw is %d' % sumbw)\n",
        "        wakatilist = wakatilistbw[:]\n",
        "        wakatilistbw = []\n",
        "        sumbw = 0\n",
        "    wakatilist.append((bfsi[1],bfsi[2]))\n",
        "    print(t1, sumfw)\n",
        "  wakati.append((bunruii, numw, wakatilist))\n",
        "  writeallwakati(bunruii, numw, wakatilist)\n",
        "  wakatilist = []\n",
        "  wakatilistbw = []\n",
        "  sumfw = 0\n",
        "  numw = 0\n",
        "  sumbw = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcvRRby9HIoM"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "%cd /content/\n",
        "!wget https://github.com/facebookresearch/fastText/archive/0.2.0.zip\n",
        "!unzip 0.2.0.zip\n",
        "%cd fastText-0.2.0\n",
        "!make\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/houreibunrui20210510/01-50wakatiall\n",
        "lswakatiall = \" \".join(['ls','*.wwakatiall'])\n",
        "output = subprocess.getoutput(lswakatiall)\n",
        "print(output)\n",
        "%cd /content/\n",
        "for fni in output.split('\\n'):\n",
        "  print(fni)\n",
        "  fno = fni + \".model\"\n",
        "  fni = \"/content/drive/My Drive/Colab Notebooks/houreibunrui20210510/01-50wakatiall/\" + fni\n",
        "  fasttextcommand = [\"/content/fastText-0.2.0/fasttext\", \"skipgram\", \"-dim\", \"128\", \"-minCount\", \"1\", \"-input\", fni, \"-output\", fno]\n",
        "  subprocess.call(fasttextcommand)\n",
        "  rmfn = fno + '.bin'\n",
        "  cpfn = fno + '.vec'\n",
        "  rmcommand = [\"rm\", rmfn]\n",
        "  cpcommand = [\"cp\", cpfn, \"/content/drive/My Drive/Colab Notebooks/houreibunrui20210510/\"]\n",
        "  subprocess.call(rmcommand)\n",
        "  subprocess.call(cpcommand)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDJY8zN8DS8b"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import math\n",
        "import json\n",
        "from pathlib import Path \n",
        "import pickle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "for fni in output.split('\\n'):\n",
        "  print(fni)\n",
        "  term_vec = []\n",
        "  fnmodelvec = fni + '.model.vec'\n",
        "  fntermvec = fni + '.term_vec.pkl'\n",
        "  with open(fnmodelvec, 'r') as f:\n",
        "    next(f)\n",
        "    for i, line in enumerate(f):\n",
        "        ents = line.split()\n",
        "        term = ' '.join(ents[:-128])\n",
        "        vec = list(map(float, ents[-128:]))\n",
        "        term_vec.append([term, i, vec])\n",
        "    open(fntermvec, 'wb').write(pickle.dumps(term_vec))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRwklbh-FvhH"
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Activation, concatenate\n",
        "from keras.layers import LSTM, GRU, SimpleRNN\n",
        "from keras.layers.core import Dropout, Reshape, Permute, RepeatVector, Flatten, Lambda\n",
        "from keras.layers.normalization import BatchNormalization as BN\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import RMSprop, Adam, Adagrad, Nadam, SGD, Adadelta, Adamax\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import glob\n",
        "from time import gmtime, strftime\n",
        "import copy\n",
        "import pickle\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import subprocess\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/houreibunrui20210510/01-50wakatiall/\n",
        "lswakatiall = \" \".join(['ls','*.wwakatiall'])\n",
        "output = subprocess.getoutput(lswakatiall)\n",
        "%cd /content/\n",
        "rng = 5\n",
        "for fni in output.split('\\n'):\n",
        "  dataset = []\n",
        "  term_vec = {}\n",
        "  ansidx = {}\n",
        "  fntermvec = fni + '.term_vec.pkl'\n",
        "  fnwakatiall = '/content/drive/MyDrive/Colab Notebooks/houreibunrui20210510/01-50wakatiall/' + fni\n",
        "  fndataset = fni + 'dataset.pkl'\n",
        "  term_vec_lists = pickle.loads(open(fntermvec, 'rb').read())\n",
        "  for term_vec_list in term_vec_lists:\n",
        "    term_vec[term_vec_list[0]] = term_vec_list[2]\n",
        "    ansidx[term_vec_list[0]] = term_vec_list[1]\n",
        "  with open(fnwakatiall, 'r') as f:\n",
        "    for fi, line in enumerate(f):\n",
        "      terms = line.split()\n",
        "      for cur in range(rng, len(terms) - rng, 1):\n",
        "        try:\n",
        "          head = list(map(lambda x:term_vec[x], terms[cur-rng:cur]))\n",
        "          ans  = terms[cur]\n",
        "          tail = list(map(lambda x:term_vec[x], terms[cur+1:cur+rng]))\n",
        "          pure_text = terms[cur-rng:cur+rng]\n",
        "          dataset.append( (head, ansidx[ans], tail, pure_text) )\n",
        "        except KeyError as e:\n",
        "          pass\n",
        "  print(fni,\" data set is %d\" % len(dataset))\n",
        "  open(fndataset, 'wb').write(pickle.dumps(dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlQS-0sYLGE3"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Activation, concatenate\n",
        "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Dropout, Reshape, Permute, RepeatVector, Flatten, Lambda\n",
        "from tensorflow.keras.layers import BatchNormalization as BN\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam, Adagrad, Nadam, SGD, Adadelta, Adamax\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/houreibunrui20210510/01-50wakatiall/\n",
        "lswakatiall = \" \".join(['ls','*.wwakatiall'])\n",
        "output = subprocess.getoutput(lswakatiall)\n",
        "print(output)\n",
        "%cd /content/\n",
        "rng = 5\n",
        "def build_model(maxlen=None, out_dim=None, in_dim=128):\n",
        "  print('Build model...')\n",
        "  model = Sequential()\n",
        "  model.add(GRU(128*rng*2, return_sequences=False, input_shape=(maxlen, in_dim)))\n",
        "  model.add(BN())\n",
        "  model.add(Dense(out_dim))\n",
        "  model.add(Activation('linear'))\n",
        "  model.add(Activation('sigmoid'))\n",
        "  optimizer = Adam()\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "  return model\n",
        "print(\"importing data from algebra...\")\n",
        "for fni in output.split('\\n'):\n",
        "  fndatasetpkl = fni + 'dataset.pkl'\n",
        "  datasets = pickle.loads(open(fndatasetpkl, 'rb').read())\n",
        "  if len(datasets) == 0:\n",
        "    continue\n",
        "  sentences = []\n",
        "  answers   = []\n",
        "  term_vec = {}\n",
        "  ansidx = {}\n",
        "  random.shuffle(datasets)\n",
        "  for dbi, series in enumerate(datasets):\n",
        "    head, ans, tail, pure_text = series \n",
        "    head.extend(tail)\n",
        "    sentences.append(np.array(head))\n",
        "    answers.append(ans)\n",
        "  print('nb sequences:', len(sentences))\n",
        "  fntermvecpkl = fni + '.term_vec.pkl'\n",
        "  term_vec_lists = pickle.loads(open(fntermvecpkl, 'rb').read())\n",
        "  for term_vec_list in term_vec_lists:\n",
        "    term_vec[term_vec_list[0]] = term_vec_list[2]\n",
        "    ansidx[term_vec_list[0]] = term_vec_list[1]\n",
        "  term_vec_lists = []\n",
        "  term_vec_list = []\n",
        "  print('Vectorization...')\n",
        "  print(len(term_vec))\n",
        "  X = np.zeros((len(sentences), len(sentences[0]), 128), dtype=np.float64)\n",
        "  print(len(sentences),len(answers))\n",
        "  y = np.zeros((len(sentences), len(term_vec)), dtype=np.int)\n",
        "  for i, sentence in enumerate(sentences):\n",
        "    if i%10000 == 0:\n",
        "      print(\"building training vector... iter %d\"%i)\n",
        "    for t, vec in enumerate(sentence):\n",
        "      X[i, t, :] = vec\n",
        "    y[i, answers[i]] = 1\n",
        "  model = build_model(maxlen=len(sentences[0]), in_dim=128, out_dim=len(term_vec))\n",
        "  for iteration in range(1, 31):\n",
        "    print()\n",
        "    print('-' * 50)\n",
        "    print('Iteration', iteration)\n",
        " \n",
        "    model.fit(X, y, batch_size=128, epochs=1)\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "    if iteration == 30:\n",
        "      MODEL_NAME = './models' + fni + \"/snapshot.%09d.model\" % iteration\n",
        "      model.save(MODEL_NAME)\n",
        "\n",
        "  del X\n",
        "  del y\n",
        "  del model\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
